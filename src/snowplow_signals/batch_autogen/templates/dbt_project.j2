{% raw %}name: '{% endraw %}{{ project_name }}{% raw %}'
version: '0.0.1'
config-version: 2

require-dbt-version: [">=1.6.0", "<2.0.0"]

profile: 'snowplow_autogen'

dispatch:
  - macro_namespace: dbt
    search_order: ['snowplow_utils', 'dbt']

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]
docs-paths: ["docs"]

target-path: "target"
clean-targets:
  - "target"
  - "dbt_packages"
  - "logs"
  
models:
  {% endraw %}{{ project_name }}{% raw %}:
    +materialized: table
    +file_format:  "{{ 'delta' if target.type not in ['spark'] else 'iceberg'}}"
    +incremental_strategy: "{{ None if target.type not in ['spark'] else 'merge' }}"
    +bind: false
    base:
      +tags: "base"
      manifest:
        +schema: "snowplow_manifest"
      scratch:
        +schema: "scratch"
        +tags: "scratch"
    filtered_events:
      +schema: "derived"
      +tags: "{% endraw %}{{ project_name }}{% raw %}_incremental"
      scratch:
        +schema: "scratch"
        +tags: "scratch"
    daily_aggregates:
      +schema: "derived"
      +tags: "{% endraw %}{{ project_name }}{% raw %}_incremental"
      scratch:
        +schema: "scratch"
        +tags: "scratch"
      manifest:
        +schema: "snowplow_manifest"
    attributes:
      +schema: "derived"
      +tags: "{% endraw %}{{ project_name }}{% raw %}_incremental"

vars:
  {% endraw %}{{ project_name }}{% raw %}:

    # OPERATION & LOGIC
    
    # filtered_events
    snowplow__start_date: '2025-01-01' # Date from which this module starts processing events (based on both load_tstamp and derived_tstamp)
    snowplow__app_id: [] # Filter the data on specific app_ids
    snowplow__backfill_limit_days: 1 # Limit backfill increments for the filtered_events_table
    
    # daily_aggregates
    snowplow__late_event_lookback_days: 5 # The number of days to allow for late arriving data to be reprocessed during daily aggregation 
    snowplow__min_late_events_to_process: 1 # The threshold number of skipped daily events to process (when reached) during daily aggregation
    
    # aggregates
    snowplow__include_current_day_in_windows: false # If set to true, the current_day with incomplete data is also taken into account for last_x_day type windows
    
    snowplow__allow_refresh: false #  If set to true, the incremental manifest will be dropped when running with a `--full-refresh` flag
    snowplow__dev_target_name: dev # If the target matches the dev target, there is no need to set the snowplow__allow_refresh to true to drop the manifest table as well
    
    # WAREHOUSE & TRACKER
    
    # snowplow__atomic_schema: 'atomic' # Only set if not using 'atomic' schema for Snowplow events data
    # snowplow__database: # Only set if not using target.database for Snowplow events data -- WILL BE IGNORED FOR DATABRICKS
    # snowplow__events_table: "events" # Only set if not using 'events' table for Snowplow events data

    # PRIVATE VARS - DO NOT MODIFY
    snowplow__databricks_catalog: "hive_metastore"
    snowplow__events: "{{ source('atomic', 'events') }}"
    snowplow__entity_key: '{% endraw %}{{ entity_key }}{% raw %}' # The entity key to use as grain for aggregations (e.g. domain_userid)

on-run-start:
  - "{{ snowplow_utils.snowplow_delete_from_manifest(var('models_to_remove',[])) }}"

on-run-end:
  - "{{ snowplow_utils.snowplow_incremental_post_hook_t(package_name='{% endraw %}{{ project_name }}{% raw %}', incremental_manifest_table_name='{% endraw %}{{ project_name }}{% raw %}_incremental_manifest', base_events_this_run_table_name='{% endraw %}{{ project_name }}{% raw %}_base_events_this_run') }}"
  - "{{ snowplow_utils.grant_usage_on_schemas_built_into(var('snowplow__grant_schemas', true)) }}"
{% endraw %}
